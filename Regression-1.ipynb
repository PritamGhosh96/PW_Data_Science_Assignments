{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cb63a4",
   "metadata": {},
   "source": [
    "###### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa244dc",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "- Models the relationship between one dependent and one independent variable.\n",
    "- Y = C0 + C1X + e\n",
    "- Simpler dealing with one relationship.\n",
    "- Suitable when there is one clear predictor.\n",
    "- Typically visualized with a 2D scatter plot and a line of best fit.\n",
    "- Less risk of overfitting\n",
    "- Ex - Prediciting House Price with only Plot Size.\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "\n",
    "- Models the relationship between one dependent and two or more independent variables.\n",
    "-  \tY = C0 + C1X1 + C2X2 + C3X3 + ….. + CnXn + e\n",
    "- More complex due to multiple relationships.\n",
    "- Suitable when multiple factors affect the outcome.\n",
    "- Requires 3D or multi-dimensional space, often represented using partial regression plots to visualize.\n",
    "- Risk of Overfitting is Higher, especially if too many predictors are used without adequate data.\n",
    "- Ex - Predicting House Price with Plot size and No. of floors and other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a2c01",
   "metadata": {},
   "source": [
    "###### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae1a3b",
   "metadata": {},
   "source": [
    " Assumptions of linear regression include:\n",
    "\n",
    "- Linearity of the Data.\n",
    "    <br>We can check the linearity of the data by looking at the Residual vs Fitted plot. Ideally, this plot would not have a pattern where the red line (lowes smoother) is approximately horizontal at zero.\n",
    "- Predictors (x) are Independent & Observed with Negligible Error\n",
    "    <br>The easiest way to check the assumption of independence is using the Durbin-Watson test. We can conduct this test using R’s built-in function called durbinWatsonTest on our model. Running this test will give you an output with a p-value, which will help determine whether the assumption is met or not. \n",
    "- Residual Errors have a Mean Value of Zero.\n",
    "    <br>We can easily check this assumption by looking at the same residual vs fitted plot. We would ideally want to see the red line flat on 0, which would indicate that the residual errors have a mean value of zero.\n",
    "- Residual Errors have Constant Variance\n",
    "    <br> We can check this assumption using the Scale-Location plot. In this plot we can see the fitted values vs the square root of the standardized residuals. Ideally, we would want to see the residual points equally spread around the red line, which would indicate constant variance.\n",
    "- Residual Errors are Independent from Each Other & Predictors (x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2204d2e",
   "metadata": {},
   "source": [
    "###### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0548fc",
   "metadata": {},
   "source": [
    "Slop informs about the rate of change in the dependant variable for a unit change in the independant variable. While the intercept gives the value of dependant variable in the abscence of the independant variables.  \n",
    "<br>\n",
    "For Example,\n",
    "Given below is the salary and Experience of a person.\n",
    "0 - 500000\n",
    "1 - 600000\n",
    "2 - 720000\n",
    "3 - 864000\n",
    "\n",
    "Here, There is a relationship between the salary and the year. As the year increases salary increases by 20% but the initial Salary 450000 is independent of the experience year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad81bd",
   "metadata": {},
   "source": [
    "###### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be95797",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent in machine learning is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92a3ef",
   "metadata": {},
   "source": [
    "###### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562578d5",
   "metadata": {},
   "source": [
    "Multiple linear regression is a regression model that estimates the relationship between a quantitative dependent variable and two or more independent variables using a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8a7c6",
   "metadata": {},
   "source": [
    "Refer Q1 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba131b8c",
   "metadata": {},
   "source": [
    "###### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbcb09",
   "metadata": {},
   "source": [
    "Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a regression model are highly correlated with each other. In other words, multicollinearity indicates a strong linear relationship among the predictor variables. This can create challenges in the regression analysis because it becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c7511",
   "metadata": {},
   "source": [
    "VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05805c24",
   "metadata": {},
   "source": [
    "One method to detect multicollinearity is to calculate the variance inflation factor (VIF) for each independent variable, and a VIF value greater than 1.5 indicates multicollinearity.\n",
    "\n",
    "To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0d61a",
   "metadata": {},
   "source": [
    "###### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99aeba",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that models the relationship between independent and dependent variables as an nth degree polynomial. It's a special case of multiple linear regression because it's linear as a statistical estimation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74401aa2",
   "metadata": {},
   "source": [
    "In polynomial regression, the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. The equation for polynomial regression is:\n",
    "    \n",
    "    y = b0 + b1x + b2x2 + b3x3 + ... + bnxn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf24f7",
   "metadata": {},
   "source": [
    "In Linear regression all the degree of the independedent variable is 1. In Polynominal Regression it is >1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9e7f0",
   "metadata": {},
   "source": [
    "###### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d69902",
   "metadata": {},
   "source": [
    "ADVANTAGES OF POLYNOMIAL REGRESSION\n",
    "- Polynomial regression is independent of the size of the data set.\n",
    "- Non-linear problems are solved with good accuracy.\n",
    "- It gives the best approximation of the correspondence between the output and explanatory (independent) variables.\n",
    "- A large number of functions can be fit under it.\n",
    "- Many curvatures can be fit by polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e4649",
   "metadata": {},
   "source": [
    "DISADVANTAGES OF POLYNOMIAL REGRESSION\n",
    "- The occurrence of one or two outliers in the data set can seriously affect the results obtained in performing the nonlinear polynomial regression analysis.\n",
    "- The polynomial regression technique is sensitive to outliers.\n",
    "- For a good bias trade-off selecting the right polynomial degree is very important in analysis.\n",
    "- There are a small number of model validation tools for the detection of outliers in non-linear regression analysis as compared to linear regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994c49f",
   "metadata": {},
   "source": [
    "Non-linear data cannot be fit by linear regression\n",
    "technique (under-fitting). So, we increase the model\n",
    "complexity and use the polynomial regression model,\n",
    "which fits such big non-linear data in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eabe55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
