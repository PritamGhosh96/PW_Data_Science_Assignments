{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7458ec3",
   "metadata": {},
   "source": [
    "###### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd345a69",
   "metadata": {},
   "source": [
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\n",
    "\n",
    "Techniques to Reduce Underfitting\n",
    "\n",
    "    Increase model complexity.\n",
    "    Increase the number of features, performing feature engineering.\n",
    "    Remove noise from the data.\n",
    "    Increase the number of epochs or increase the duration of training to get better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08164acf",
   "metadata": {},
   "source": [
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
    "\n",
    "Techniques to Reduce Overfitting\n",
    "\n",
    "    Increase training data.\n",
    "    Reduce model complexity.\n",
    "    Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "    Ridge Regularization and Lasso Regularization.\n",
    "    Use dropout for neural networks to tackle overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42edc7a",
   "metadata": {},
   "source": [
    "###### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e03fbe",
   "metadata": {},
   "source": [
    "there are some ways by which we can reduce the occurrence of overfitting in our model.\n",
    "\n",
    "    Cross-Validation\n",
    "    Training with more data\n",
    "    Removing features\n",
    "    Early stopping the training\n",
    "    Regularization\n",
    "    Ensembling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010497be",
   "metadata": {},
   "source": [
    "###### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989d01e",
   "metadata": {},
   "source": [
    "Underfitting becomes obvious when the model is too simple and cannot create a relationship between the input and the output. It is detected when the training error is very high and the model is unable to learn from the training data. High bias and low variance are the most common indicators of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ab7b6",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355cf3d",
   "metadata": {},
   "source": [
    "###### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70cd4d6",
   "metadata": {},
   "source": [
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time. For the graph, the perfect tradeoff will be like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57785cfe",
   "metadata": {},
   "source": [
    "###### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd8f79",
   "metadata": {},
   "source": [
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. <br>A model is underfitting the training data when the model performs poorly on the training data. A model is overfitting the training data when the model performs very good on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5226d3",
   "metadata": {},
   "source": [
    "###### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdea17c",
   "metadata": {},
   "source": [
    "Definition\t\n",
    "\n",
    "Bias - When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations.\t<br>Variance - The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data.\n",
    "\n",
    "Values\n",
    "\n",
    "Bias - The disparity between the values that were predicted and the values that were actually observed is referred to as bias.<br>Variance - A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "\n",
    "Data\t\n",
    "\n",
    "Bias - The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data.\t<br>Variance - The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f57516",
   "metadata": {},
   "source": [
    "High bias value means more assumptions are taken to build the target function. In this case, the model will not match the training dataset closely. <br> High variance means that the model is very sensitive to changes in the training data and can result in significant changes in the estimate of the target function when trained on different subsets of data from the same distribution. This is the case of overfitting when the model performs well on the training data but poorly on new, unseen test data. It fits the training data too closely that it fails on the new training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0068b",
   "metadata": {},
   "source": [
    "###### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4294c3",
   "metadata": {},
   "source": [
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14147e8",
   "metadata": {},
   "source": [
    "There are mainly two types of regularization techniques, which are given below:\n",
    "\n",
    "    Ridge Regression\n",
    "    Lasso Regression\n",
    "\n",
    "Ridge Regression\n",
    "\n",
    "    Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\n",
    "    Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\n",
    "    In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
    "    \n",
    "Lasso Regression:\n",
    "\n",
    "    Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\n",
    "    It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
    "    Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef24255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
