{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bb50ed",
   "metadata": {},
   "source": [
    "###### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569b917",
   "metadata": {},
   "source": [
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df1f59",
   "metadata": {},
   "source": [
    "R^2 = 1 - (ssr/sst) = 1 - (sum((y - y_pred)^2)/(sum((y - y_mean)^2)\n",
    "\n",
    "The sum squared regression is the sum of the residuals squared, and the total sum of squares is the sum of the distance the data is away from the mean all squared. It represent how much the model is predicting the data correctly in decimal format . The value ranges from 0 -1 . 0 being 0% and 1 being 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f3ece",
   "metadata": {},
   "source": [
    "###### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44981ebf",
   "metadata": {},
   "source": [
    "The adjusted R-squared is a modified version of R-squared that adjusts for predictors that are not significant in a regression model. Compared to a model with additional input variables, a lower adjusted R-squared indicates that the additional input variables are not adding value to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1970d3",
   "metadata": {},
   "source": [
    "Here are the differences between R-square and Adjusted R-square:\n",
    "\n",
    "    - Every time we add a independent variable to a model, the R-squared increases, even if the independent variable is insignificant. It never declines. Whereas Adjusted R-squared increases only when independent variable is significant and affects dependent variable.\n",
    "    - Adjusted r-squared can be negative when r-squared is close to zero.\n",
    "    - Adjusted r-squared value always be less than or equal to r-squared value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b09d5f",
   "metadata": {},
   "source": [
    "###### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b2984",
   "metadata": {},
   "source": [
    "Adjusted R -square is used to find the impact of adding a new feature to the model. To check the model efficiency before and after on a correct scale adjusted R-square is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394ea43",
   "metadata": {},
   "source": [
    "###### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d81b3d",
   "metadata": {},
   "source": [
    "The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "\n",
    "MAE = sum(abs(y-y_pred))/N\n",
    "\n",
    "<br>Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
    "\n",
    "MSE = sum((y-y_pred)^2)/N\n",
    "\n",
    "<br>Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
    "\n",
    "RMSE = (sum((y-y_pred)^2)/N)^0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3c584",
   "metadata": {},
   "source": [
    "###### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78799b90",
   "metadata": {},
   "source": [
    "MSE\n",
    "\n",
    "Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function.\n",
    "\n",
    "Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error. Yet in many practical cases we don’t care much about these outliers and are aiming for more of a well-rounded model that performs good enough on the majority.\n",
    "\n",
    "MAE\n",
    "\n",
    "Advantage: The beauty of the MAE is that its advantage directly covers the MSE disadvantage. Since we are taking the absolute value, all of the errors will be weighted on the same linear scale. Thus, unlike the MSE, we won’t be putting too much weight on our outliers and our loss function provides a generic and even measure of how well our model is performing.\n",
    "\n",
    "Disadvantage: If we do in fact care about the outlier predictions of our model, then the MAE won’t be as effective. The large errors coming from the outliers end up being weighted the exact same as lower errors. This might results in our model being great most of the time, but making a few very poor predictions every so-often.\n",
    "\n",
    "RMSE\n",
    "\n",
    "Advantage: Has the same unit as the original data\n",
    "\n",
    "Disadvantage: Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a9505",
   "metadata": {},
   "source": [
    "###### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b1f24",
   "metadata": {},
   "source": [
    "The LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero. The feature selection phase occurs after the shrinkage, where every non-zero value is selected to be used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f818dbc",
   "metadata": {},
   "source": [
    "\n",
    "    Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.\n",
    "    Lasso: Along with shrinking coefficients, the lasso also performs feature selection. Some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0260417",
   "metadata": {},
   "source": [
    "Use Lasso when feature selection is crucial or when you want a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a7bba",
   "metadata": {},
   "source": [
    "###### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962b39b",
   "metadata": {},
   "source": [
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation. Thus, if the coefficient inflates, the cost function will increase. And Linear regression model will try to optimize the coefficient in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462eba5a",
   "metadata": {},
   "source": [
    "###### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7f177",
   "metadata": {},
   "source": [
    "The limitations of Regularization are:\n",
    "\n",
    "    Regularization can make the model too simple and unsuitable for the data if the regularization parameter is too large. This can cause the model to miss important patterns in the data.\n",
    "    Regularization assumes that all input variables are of equal importance, which may not be true in some cases. Using domain knowledge to weigh the importance of input variables can help improve model performance.\n",
    "    Regularization can be computationally expensive, especially for multidimensional data. This can make it difficult to use normalization in real-time applications.\n",
    "    Regularization assumes that the data is linearly separable, which may not be true in some cases. Non-linear models, such as neural networks, maybe a better fit for that data.\n",
    "    Regularization assumes that the data are independent and identically distributed (IIDs), which may not be true for some types of data, such as time series or spatial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06360f4d",
   "metadata": {},
   "source": [
    "###### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60410fc8",
   "metadata": {},
   "source": [
    "Comparing the two models:\n",
    "\n",
    "- Model A has a lower RMSE (10), indicating that it might perform better when large errors are of particular concern.\n",
    "- Model B has a lower MAE (8), suggesting that it might perform better in terms of overall average prediction accuracy.\n",
    "\n",
    "Since both RMSE and MAE have their strengths and weaknesses, the choice of the better model depends on the specific goals of the analysis and the relative importance of minimizing large errors versus achieving a more balanced average prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a45b66",
   "metadata": {},
   "source": [
    "###### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead1bb1",
   "metadata": {},
   "source": [
    "In ridge regression and lasso, lambda (λ) is a hyperparameter that controls the strength of the regularization penalty applied to the coefficients of the model. In both ridge regression and lasso, the addition of the regularization term helps to prevent overfitting by penalizing large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845b6f8",
   "metadata": {},
   "source": [
    "As Model A uses Ridge regularization with a regularization parameter of 0.1 that means the magnitude of penalty is less than Model B which uses Lasso regularization with a regularization parameter of 0.5. It implies the Overfitness in Model a is still more than that of Model B. So I'll go for Model B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e66b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
